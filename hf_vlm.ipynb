{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNQ73jTpNN9p75byx3Dsi0K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satojkovic/hf_vlm/blob/main/hf_vlm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "OEOClDYLt_M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TmLYCu1sd2X"
      },
      "outputs": [],
      "source": [
        "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", token=userdata.get('HF_TOKEN'))\n",
        "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
        "    \"llava-hf/llava-v1.6-mistral-7b-hf\",\n",
        "    torch_dtype=torch.float16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    token=userdata.get('HF_TOKEN')\n",
        ")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "prompt = \"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n",
        "\n",
        "inputs = processor(image, prompt, return_tensors=\"pt\").to(device)\n",
        "output = model.generate(**inputs, max_new_tokens=100)"
      ],
      "metadata": {
        "id": "s2xfModusd2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(processor.decode(output[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "dTGYlwd6uQsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SFT using TRL\n",
        "\n",
        "The full example script can be found in sft_vlm.py\n",
        "\n",
        "rename examples/scripts/{vsft_llava.py => sft_vlm.py}"
      ],
      "metadata": {
        "id": "uLMZQgCQFttK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes"
      ],
      "metadata": {
        "id": "n9-FBJeTRRXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U trl"
      ],
      "metadata": {
        "id": "ie765p3VuVkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError(\"GPU not available. Please change runtime type to include a GPU.\")\n",
        "else:\n",
        "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n"
      ],
      "metadata": {
        "id": "JUz26OpvHOsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 3. Import required libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForVision2Seq, AutoProcessor, LlavaForConditionalGeneration\n",
        "from trl import (\n",
        "    ModelConfig,\n",
        "    SFTConfig,\n",
        "    SFTTrainer,\n",
        "    get_kbit_device_map,\n",
        "    get_peft_config,\n",
        "    get_quantization_config,\n",
        ")\n",
        "import os"
      ],
      "metadata": {
        "id": "NKwTE3iFFzZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Setting parameters\n",
        "# --------------------------------------------------------------------------------\n",
        "# Script Arguments\n",
        "# --------------------------------------------------------------------------------\n",
        "dataset_name: str = \"HuggingFaceH4/llava-instruct-mix-vsft\" # 元のデータセット\n",
        "# dataset_name: str = \"ydshieh/llava-chat-hf-subsample-blip-caption\" # より小さいサブサンプルデータセット (テスト用)\n",
        "# dataset_name: str = \"liuhaotian/LLaVA-Pretrain\" # LLaVA論文のデータセットの一つ（形式確認が必要）\n",
        "\n",
        "dataset_train_split: str = \"train\"\n",
        "dataset_test_split: str = \"test\" # testスプリットがないデータセットの場合、eval_strategy=\"no\" にするか、trainスプリットを分割する必要がある\n"
      ],
      "metadata": {
        "id": "eXe6Mwb4HKza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------------\n",
        "# Model Arguments\n",
        "# --------------------------------------------------------------------------------\n",
        "# model_name_or_path: str = \"llava-hf/llava-1.5-7b-hf\" # 元のモデル\n",
        "model_name_or_path: str = \"llava-hf/llava-1.5-7b-hf\" # 小さいモデルでテストしたい場合は変更\n",
        "# model_name_or_path: str = \"llava-hf/llava-v1.6-mistral-7b-hf\" # Transformers >= 4.45\n",
        "# model_name_or_path: str = \"meta-llama/Llama-3.2-11B-Vision-Instruct\" # Transformers >= 4.45.1 (Colab無料枠ではメモリ不足の可能性大)\n",
        "\n",
        "model_revision: str = None # \"main\" or specific commit hash\n",
        "attn_implementation: str = None # \"flash_attention_2\" or None. Noneで自動選択。T4ではFlashAttention2は非対応の場合が多い\n",
        "torch_dtype_str: str = \"bfloat16\" # \"bfloat16\", \"float16\", \"auto\". T4はbfloat16をサポート\n",
        "trust_remote_code: bool = True # LLaVAモデルではTrueが必要な場合が多い\n",
        "\n",
        "# PEFT (LoRA) Configuration - Colabでのメモリ削減のためLoRAを有効化\n",
        "use_peft: bool = True\n",
        "peft_lora_r: int = 16 # LoRA rank\n",
        "peft_lora_alpha: int = 32 # LoRA alpha\n",
        "# peft_target_modules: list = None # 自動検出に任せるか、モデルに合わせて指定 [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "# LLaVA-1.5 (Llamaベース) の一般的なターゲットモジュール\n",
        "peft_target_modules: list = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        "\n",
        "# Quantization Configuration (QLoRAを使う場合)\n",
        "# load_in_4bit: bool = True # QLoRA (4-bit quantization) を使う場合はTrue\n",
        "# bnb_4bit_quant_type: str = \"nf4\" # \"nf4\" or \"fp4\"\n",
        "# bnb_4bit_compute_dtype_str: str = \"bfloat16\" # \"bfloat16\", \"float16\"\n",
        "load_in_4bit: bool = False # まずはLoRA + bf16で試す\n",
        "bnb_4bit_quant_type: str = \"nf4\"\n",
        "bnb_4bit_compute_dtype_str: str = \"bfloat16\""
      ],
      "metadata": {
        "id": "Js0Ng8VFJzO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------------\n",
        "# SFT Training Arguments (Hugging Face TrainingArguments)\n",
        "# --------------------------------------------------------------------------------\n",
        "output_dir: str = \"sft-llava-colab-test\"\n",
        "# per_device_train_batch_size: int = 8 # 元の設定\n",
        "# gradient_accumulation_steps: int = 8 # 元の設定\n",
        "per_device_train_batch_size: int = 1 # Colab T4 (16GB VRAM) のメモリ制約のため削減\n",
        "gradient_accumulation_steps: int = 16 # 実効バッチサイズを 1 * 16 = 16 に維持 (元の 8*8=64 よりは小さい)\n",
        "\n",
        "# num_train_epochs: float = 3.0 # 元のデフォルト\n",
        "num_train_epochs: float = 1.0 # テストのため短縮\n",
        "learning_rate: float = 1e-4 # LoRAの場合、少し高めの学習率が有効なことがある\n",
        "lr_scheduler_type: str = \"cosine\"\n",
        "optim: str = \"paged_adamw_8bit\" if load_in_4bit or use_peft else \"adamw_torch\" # QLoRA/LoRAならpaged_adamw_8bit\n",
        "\n",
        "logging_steps: int = 10\n",
        "# eval_strategy: str = \"steps\" # testスプリットがある場合\n",
        "eval_strategy: str = \"no\" # 小さいテストデータセットにはtestスプリットがない場合があるため \"no\" に。あれば \"steps\"\n",
        "# eval_steps: int = 100 # eval_strategy=\"steps\" の場合\n",
        "save_strategy: str = \"steps\"\n",
        "save_steps: int = 200 # 進行状況を保存する頻度\n",
        "# save_total_limit: int = 1 # 古いチェックポイントを削除する場合\n",
        "\n",
        "bf16: bool = (torch_dtype_str == \"bfloat16\" and not load_in_4bit) # QLoRAの場合、bf16はcompute_dtypeで指定\n",
        "fp16: bool = (torch_dtype_str == \"float16\" and not load_in_4bit) # QLoRAの場合、fp16はcompute_dtypeで指定\n",
        "\n",
        "gradient_checkpointing: bool = True\n",
        "# gradient_checkpointing_kwargs = dict(use_reentrant=False) # スクリプト通り\n",
        "# remove_unused_columns = False # スクリプト通り\n",
        "# dataset_kwargs = {\"skip_prepare_dataset\": True} # スクリプト通り\n",
        "\n",
        "report_to: str = \"none\" # \"tensorboard\", \"wandb\", \"none\"\n",
        "push_to_hub: bool = False\n",
        "# hub_model_id: str = \"your-username/sft-llava-colab\" # push_to_hub=True の場合に設定\n"
      ],
      "metadata": {
        "id": "PTAAtnesJ2zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------------------------------------------\n",
        "# Instantiate Config Objects\n",
        "# --------------------------------------------------------------------------------\n",
        "model_args = ModelConfig(\n",
        "    model_name_or_path=model_name_or_path,\n",
        "    model_revision=model_revision,\n",
        "    attn_implementation=attn_implementation,\n",
        "    torch_dtype=torch_dtype_str if not load_in_4bit else None, # QLoRAの場合はNoneにし、quantization_configで設定\n",
        "    trust_remote_code=trust_remote_code,\n",
        "    use_peft=use_peft,\n",
        "    lora_r=peft_lora_r,\n",
        "    lora_alpha=peft_lora_alpha,\n",
        "    # peft_target_modules=peft_target_modules, # 指定する場合\n",
        "    lora_target_modules=peft_target_modules,\n",
        "    load_in_4bit=load_in_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        ")"
      ],
      "metadata": {
        "id": "PzOiP2slJ7qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = SFTConfig(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    learning_rate=learning_rate,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    optim=optim,\n",
        "    logging_steps=logging_steps,\n",
        "    # eval_steps=eval_steps if eval_strategy == \"steps\" else None,\n",
        "    save_strategy=save_strategy,\n",
        "    save_steps=save_steps,\n",
        "    # save_total_limit=save_total_limit,\n",
        "    bf16=bf16,\n",
        "    fp16=fp16,\n",
        "    gradient_checkpointing=gradient_checkpointing,\n",
        "    gradient_checkpointing_kwargs=dict(use_reentrant=False), # 元のスクリプトから\n",
        "    remove_unused_columns=False, # 元のスクリプトから\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True}, # 元のスクリプトから\n",
        "    report_to=report_to,\n",
        "    push_to_hub=push_to_hub,\n",
        "    # hub_model_id=hub_model_id if push_to_hub else None,\n",
        "    # max_seq_length=2048, # 必要に応じて設定 (SFTTrainerのデフォルトは1024)\n",
        ")"
      ],
      "metadata": {
        "id": "grd1AMFfJ_yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 5. Main process (Load model / Data preparation / Training)\n",
        "\n",
        "################\n",
        "# Model, Tokenizer & Processor\n",
        "################\n",
        "torch_dtype = (\n",
        "    model_args.torch_dtype if model_args.torch_dtype in [\"auto\", None] else getattr(torch, model_args.torch_dtype)\n",
        ")\n",
        "quantization_config = get_quantization_config(model_args) # model_argsから4-bit/8-bit設定を読み込む\n"
      ],
      "metadata": {
        "id": "ua5ZSEKpKlgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine device_map\n",
        "if quantization_config is not None:\n",
        "    # For quantized models (4-bit/8-bit), get_kbit_device_map handles device placement\n",
        "    device_map_value = get_kbit_device_map()\n",
        "    print(f\"Quantization is enabled. Using device_map from get_kbit_device_map(): {device_map_value}\")\n",
        "elif torch.cuda.is_available():\n",
        "    # If not quantized and GPU is available, set to \"auto\"\n",
        "    # \"auto\" lets accelerate handle device placement (typically to cuda:0 on single GPU)\n",
        "    device_map_value = \"auto\"\n",
        "    print(f\"GPU is available. Setting device_map='{device_map_value}' for non-quantized model.\")\n",
        "else:\n",
        "    # No quantization and no GPU, so device_map will be None (CPU)\n",
        "    device_map_value = None\n",
        "    print(\"GPU not available or quantization not used. Model will be loaded on CPU if device_map is None.\")"
      ],
      "metadata": {
        "id": "TciIa27qL68V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_kwargs = dict(\n",
        "    revision=model_args.model_revision,\n",
        "    attn_implementation=model_args.attn_implementation,\n",
        "    torch_dtype=torch_dtype,\n",
        "    device_map=device_map_value, # Use the determined device_map_value\n",
        "    quantization_config=quantization_config,\n",
        ")\n",
        "# QLoRAの場合、torch_dtypeはquantization_configで指定されるため、明示的に渡すとエラーになることがあるので削除\n",
        "if quantization_config is not None:\n",
        "    if 'torch_dtype' in model_kwargs:\n",
        "        del model_kwargs['torch_dtype'] # get_quantization_configが内部でdtypeを扱うため"
      ],
      "metadata": {
        "id": "V32oX3wxK2xE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading processor...\")\n",
        "processor = AutoProcessor.from_pretrained(\n",
        "    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code\n",
        ")"
      ],
      "metadata": {
        "id": "84vh9XmAK7UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading model...\")\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "R0qM1QzLK9Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルがロードされたデバイスを確認\n",
        "if hasattr(model, 'device'):\n",
        "    print(f\"Model loaded on device: {model.device}\")\n",
        "else: # PeftModelの場合\n",
        "    for n, p in model.named_parameters():\n",
        "        print(f\"Parameter {n} on device: {p.device}\")\n",
        "        break"
      ],
      "metadata": {
        "id": "f03090QVK_lE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "# Create a data collator to encode text and image pairs\n",
        "################\n",
        "def collate_fn(examples):\n",
        "    texts = []\n",
        "    images_batch = [] # Changed variable name to avoid conflict\n",
        "    for example in examples:\n",
        "        # messagesが文字列の場合とリストの場合に対応\n",
        "        if isinstance(example[\"messages\"], str): # 一部のデータセットは単一文字列でテキストを持つ場合がある\n",
        "            # この場合、チャットテンプレートを適用するなら、適切な形式に変換する必要がある\n",
        "            # 例: [{\"role\": \"user\", \"content\": example[\"messages\"]}]\n",
        "            # ここでは、データセットが期待する形式であることを前提とする\n",
        "            # もしHuggingFaceH4/llava-instruct-mix-vsft形式なら、messagesはリストのはず\n",
        "            # テスト用データセット `ydshieh/llava-chat-hf-subsample-blip-caption` は 'text' フィールドを持つ\n",
        "            if \"text\" in example and \"images\" in example: # For ydshieh's dataset\n",
        "                 # ydshieh/llava-chat-hf-subsample-blip-caption's 'text' field is like \"USER: <image>\\nWhat is this? ASSISTANT: This is a cat.\"\n",
        "                 # We need to convert this to messages format.\n",
        "                 # This is a simplified conversion, actual conversion might need more robust parsing.\n",
        "                parts = example[\"text\"].split(\"ASSISTANT:\")\n",
        "                user_content = parts[0].replace(\"USER:\", \"\").strip()\n",
        "                assistant_content = parts[1].strip() if len(parts) > 1 else \"\"\n",
        "                # <image> トークンはprocessor.apply_chat_templateが処理することを期待\n",
        "                # user_content = user_content.replace(\"<image>\\n\", processor.image_token + \"\\n\")\n",
        "                # LLaVAは <image> をメッセージの先頭に置くことが多い\n",
        "                # ユーザーの入力に <image> が含まれていることを想定\n",
        "                example_messages = [{\"role\": \"user\", \"content\": user_content}]\n",
        "                if assistant_content:\n",
        "                    example_messages.append({\"role\": \"assistant\", \"content\": assistant_content})\n",
        "            else: # Assuming example[\"messages\"] is the correct field\n",
        "                example_messages = example[\"messages\"]\n",
        "        else:\n",
        "            example_messages = example[\"messages\"]\n",
        "\n",
        "        processed_text = processor.apply_chat_template(example_messages, tokenize=False)\n",
        "        texts.append(processed_text)\n",
        "        images_batch.append(example[\"images\"]) # imagesはリストのリストになる\n",
        "\n",
        "    # LLaVA 1.5 (LlavaForConditionalGeneration) は複数画像をサポートしないため、各サンプルから最初の画像のみを使用\n",
        "    # LLaVA NeXT (LlavaNextForConditionalGeneration) などは複数画像をサポートする場合がある\n",
        "    # その場合はこの処理をモデルタイプに応じて変更する必要がある\n",
        "    # AutoModelForVision2Seqでロードした場合、具体的なモデルクラスで判定\n",
        "    if isinstance(model, LlavaForConditionalGeneration):\n",
        "        final_images = [img_list[0] for img_list in images_batch if img_list] # 各サンプルの画像リストから最初の画像を取得\n",
        "    else:\n",
        "        # 他のモデル (例: LLaVA-NeXT) が複数画像を扱える場合、そのまま渡すか、モデルの期待する形式に合わせる\n",
        "        # ここでは、データセットが各サンプルに1枚の画像リストを持つと仮定し、それを展開\n",
        "        final_images = [img_list[0] for img_list in images_batch if img_list] # シンプルに最初の画像を使う\n",
        "\n",
        "    try:\n",
        "        batch = processor(text=texts, images=final_images, return_tensors=\"pt\", padding=True)\n",
        "    except Exception as e:\n",
        "        print(\"Error during processor call. Details:\")\n",
        "        print(f\"Texts: {texts}\")\n",
        "        print(f\"Number of final images: {len(final_images)}\")\n",
        "        # print(f\"Final images: {final_images}\") # PIL Imageオブジェクトなので表示は省略\n",
        "        raise e\n",
        "\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "    # Pad tokenのマスク\n",
        "    if processor.tokenizer.pad_token_id is not None:\n",
        "        labels[labels == processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "    # Image tokenのマスク (モデル特有の処理)\n",
        "    # LlavaProcessorには image_token があるが、他のプロセッサでは異なる場合がある\n",
        "    if hasattr(processor, 'image_token'):\n",
        "        image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.image_token)\n",
        "        labels[labels == image_token_id] = -100\n",
        "    elif hasattr(processor.tokenizer, 'additional_special_tokens_ids'):\n",
        "        # Llama-3.2-Vision-Instructの場合、<image>はspecial tokenとして扱われる\n",
        "        # Idefics2プロセッサの場合、image_token_id は通常 -100 で、\n",
        "        # <image> はテキスト中に複数回出現しうる。\n",
        "        # <fake_token_around_image>のような特殊トークンもマスク対象になることがある。\n",
        "        # processor.tokenizer.special_tokens_map から '<image>' に対応するIDを取得する。\n",
        "        # もしくは、モデルのconfigから image_token_index を参照する。\n",
        "        # For Llama-3.2-Vision, it seems '<image>' is one of the special tokens.\n",
        "        # The processor might handle image tokens differently, often by inserting placeholders that are later replaced by image embeddings.\n",
        "        # Let's try to find the ID for a generic image placeholder if not `processor.image_token`\n",
        "        img_placeholder_id = None\n",
        "        if \"<image>\" in processor.tokenizer.get_vocab():\n",
        "             img_placeholder_id = processor.tokenizer.convert_tokens_to_ids(\"<image>\")\n",
        "        elif \"image_token_index\" in model.config.to_dict(): # e.g. Idefics\n",
        "             img_placeholder_id = model.config.image_token_index\n",
        "\n",
        "        if img_placeholder_id is not None:\n",
        "            labels[labels == img_placeholder_id] = -100\n",
        "            print(f\"Masked image placeholder token ID: {img_placeholder_id}\")\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ],
      "metadata": {
        "id": "SFSQeynbLDxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################\n",
        "# Dataset\n",
        "################\n",
        "print(f\"Loading dataset: {dataset_name}\")\n",
        "# HuggingFaceH4/llava-instruct-mix-vsft は images フィールドが PIL Imageのリスト\n",
        "# ydshieh/llava-chat-hf-subsample-blip-caption は images フィールドが PIL Image (リストではない)\n",
        "# データセットの形式に合わせて前処理を調整\n",
        "raw_dataset = load_dataset(dataset_name) # name=script_args.dataset_config (H4データセットはconfigなし)\n"
      ],
      "metadata": {
        "id": "7EBNoHi6MaYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ydshieh/llava-chat-hf-subsample-blip-caption の場合、'images'は単一のPIL Image\n",
        "# collate_fn が images_batch.append(example[\"images\"]) を期待するため、リストにラップする\n",
        "# また、'messages'フィールドがないので、'text'から変換する処理をcollate_fnに実装\n",
        "# def preprocess_dataset(example):\n",
        "#     if \"text\" in example and \"images\" in example: # For ydshieh's dataset\n",
        "#         # 'messages' フィールドは collate_fn で 'text' から生成するのでここでは何もしない\n",
        "#         # 'images' をリストにする\n",
        "#         if not isinstance(example[\"images\"], list):\n",
        "#             example[\"images\"] = [example[\"images\"]]\n",
        "#     # H4データセットの場合、'messages'と'images' (リスト) が存在するはず\n",
        "#     return example\n",
        "\n",
        "# dataset = raw_dataset.map(preprocess_dataset, batched=False)\n",
        "dataset = raw_dataset"
      ],
      "metadata": {
        "id": "3u96VwrfMeEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの分割を確認\n",
        "print(\"Dataset structure:\", dataset)\n",
        "if dataset_train_split not in dataset:\n",
        "    print(f\"Warning: '{dataset_train_split}' split not found in dataset. Available splits: {list(dataset.keys())}\")\n",
        "    # フォールバックとして最初のスプリットを使用するか、エラーにする\n",
        "    if list(dataset.keys()):\n",
        "        actual_train_split_name = list(dataset.keys())[0]\n",
        "        print(f\"Using '{actual_train_split_name}' as train split instead.\")\n",
        "        train_dataset = dataset[actual_train_split_name]\n",
        "    else:\n",
        "        raise ValueError(\"No splits found in the loaded dataset.\")\n",
        "else:\n",
        "    train_dataset = dataset[dataset_train_split]"
      ],
      "metadata": {
        "id": "vDiCa0M9PFb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = None\n",
        "if dataset_test_split not in dataset:\n",
        "    print(f\"Warning: '{dataset_test_split}' split not found. Disabling evaluation.\")\n",
        "    training_args.evaluation_strategy = \"no\"\n",
        "else:\n",
        "    eval_dataset = dataset[dataset_test_split]\n"
      ],
      "metadata": {
        "id": "jRHt2XHoPmqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset"
      ],
      "metadata": {
        "id": "FzMEjcdUPwKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "if eval_dataset:\n",
        "    print(f\"Eval dataset size: {len(eval_dataset)}\")\n"
      ],
      "metadata": {
        "id": "faLYjuc1P01p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################\n",
        "# Training\n",
        "################\n",
        "print(\"Initializing SFTTrainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    peft_config=get_peft_config(model_args) if model_args.use_peft else None,\n",
        "    # max_seq_length=training_args.max_seq_length, # SFTConfigで設定されていれば不要\n",
        ")"
      ],
      "metadata": {
        "id": "RELLr3jPP4S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"Training finished.\")"
      ],
      "metadata": {
        "id": "Z3yCGXWYP6__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nAyEW0CaRFuN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}